@article{Chinco2015,
abstract = {How do arbitrageurs find variables that predict returns? If a predictor lasts 30 days or more, then a clever arbitrageur can use his intuition to get the job done. But, what's an arbitrageur supposed to do if a predictor lasts 30 minutes or less? An arbitrageur's intuition is useless if the predictor decays before he can finish his morning coffee. Motivated by this observation, we show how arbitrageurs can find these sorts of rare, short-lived, “sparse” predictors by replacing intuition with a statistical procedure known as the LASSO. Using the LASSO boosts out-of-sample predictability in 1-minute returns by 23{\%} relative to standard OLS-regression models. This out-of-sample predictive power comes from quickly identifying the right predictors at the right time, not from better estimating the effects of some new factor. What's more, the predictors chosen by the LASSO correspond to real-world events: the lagged returns of stocks with announcements are 18.3{\%} more likely to be used by the LASSO as predictors.},
author = {Chinco, Alexander and Clark-Joseph, Adam D and Ye, Mao},
doi = {10.2139/ssrn.2606396},
file = {:C$\backslash$:/Users/Administrator/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chinco, Clark-Joseph, Ye - 2015 - Sparse Signals in the Cross-Section of Returns.pdf:pdf},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
keywords = {C55,C58,G12,G14,Out-of-Sample Fit,Return Predictability,Sparsity,The LASSO},
number = {916},
pages = {1--52},
title = {{Sparse Signals in the Cross-Section of Returns}},
url = {http://www.ssrn.com/abstract=2934020 http://www.ssrn.com/abstract=2606396},
year = {2015}
}
@article{Feng2017,
abstract = {The asset pricing literature has produced hundreds of potential risk factors. Organizing this “zoo of factors" and distinguishing between useful, useless, and redundant factors require econometric techniques that can deal with the curse of dimensionality. We propose a model-selection method to systematically evaluate the contribution to asset pricing of any new factor, above and beyond what a high-dimensional set of existing factors explains. Our methodology explicitly accounts for potential model-selection mistakes, unlike the standard approaches that assume perfect variable selection, which rarely occurs in finite sample and produces a bias due to the omitted variables. We apply our procedure to a set of factors recently discovered in the literature. We show that several factors - such as profitability and investments - have statistically significant explanatory power beyond the hundreds of factors proposed in the past. In addition, we show that our risk price estimates and their significance are stable, whereas the model selected by simple LASSO is not.},
author = {Feng, Guanhao and Giglio, Stefano},
doi = {10.2139/ssrn.2934020},
file = {:C$\backslash$:/Users/Administrator/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Feng, Giglio - 2017 - Taming the Factor Zoo.pdf:pdf},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
keywords = {Elastic Net,Factors,LASSO,Machine Learning,Post-Selection Inference,Regularized Two-Pass Estimation,Risk Price},
pages = {1--69},
title = {{Taming the Factor Zoo}},
url = {http://www.ssrn.com/abstract=2934020},
year = {2017}
}
@article{Gu2018,
abstract = {We synthesize the field of machine learning with the canonical problem of empirical asset pricing: Measuring asset risk premia. In the familiar empirical setting of cross section and time series stock return prediction, we perform a comparative analysis of methods in the machine learning repertoire, including generalize linear models, dimension reduction, boosted regression trees, random forests, and neural networks. At the broadest level, we find that machine learning offers an improved description of asset price behavior relative to traditional methods. Our implementation establishes a new standard for accuracy in measuring risk premia summarized by unprecedented high out-of-sample return prediction R2. We identify the best performing methods (trees and neural nets) and trace their predictive gains to allowance of nonlinear predictor interactions that are missed by other methods. Lastly, we find that all methods agree on the same small set of dominant predictive signals that includes variations on momentum, liquidity, and volatility. Improved risk premia measurement through machine learning can simplify the investigation into economic mechanisms of asset pricing and justifies its growing role in innovative financial technologies.},
author = {Gu, Shihao and Kelly, Bryan T. and Xiu, Dacheng},
doi = {10.2139/ssrn.3159577},
file = {:C$\backslash$:/Users/Administrator/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gu, Kelly, Xiu - 2018 - Empirical Asset Pricing via Machine Learning(2).pdf:pdf},
issn = {1556-5068},
journal = {Ssrn},
keywords = {(Deep) Neural Networks,(Group) Lasso,Cross-Section of Returns,Elastic Net,Fintech,Gradient Boosting,Machine Learning,Random Forest,Return Prediction,Ridge Regression},
title = {{Empirical Asset Pricing via Machine Learning}},
year = {2018}
}
@article{LopezdePrado2018,
abstract = {Over the past 20 years, I have seen many new faces arrive to the financial industry, only to leave shortly after. • The rate of failure is particularly high in machine learning (ML). • In my experience, the reasons boil down to 7 common errors: 1. The Sisyphus paradigm 2. Integer differentiation 3. Inefficient sampling 4. Wrong labeling 5. Weighting of non-IID samples 6. Cross-validation leakage 7. Backtest overfitting},
author = {{Lopez de Prado}, Marcos},
doi = {10.2139/ssrn.3104816},
file = {:C$\backslash$:/Users/Administrator/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lopez de Prado - 2018 - The 10 Reasons Most Machine Learning Funds Fail(2).pdf:pdf},
issn = {1556-5068},
journal = {Journal of Portfolio Management},
keywords = {Machine learning,backtest overfitting,investment strategies,quantamental investing},
title = {{The 10 Reasons Most Machine Learning Funds Fail}},
year = {2018}
}
@techreport{Freyberger2017,
abstract = {We propose a nonparametric method to study which characteristics provide incremental information for the cross-section of expected returns. We use the adaptive group LASSO to select characteristics and to estimate how they affect expected returns nonparametrically. Our method can handle a large number of characteristics, allows for a flexible functional form, and our implementation is insensitive to outliers. Many of the previously identified return predictors don't provide incremental information for expected returns, and nonlinearities are important. We study the properties of our method in simulations and find large improvements both in model selection and prediction compared to alternative selection methods.},
address = {Cambridge, MA},
author = {Freyberger, Joachim and Neuhierl, Andreas and Weber, Michael},
booktitle = {Revue Medicale Suisse},
doi = {10.3386/w23227},
file = {:C$\backslash$:/Users/Administrator/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Freyberger, Neuhierl, Weber - 2017 - Dissecting Characteristics Nonparametrically.pdf:pdf},
institution = {National Bureau of Economic Research},
issn = {16609379},
keywords = {anomalies,cross section of returns,expected returns,model},
month = {mar},
number = {430},
pages = {1092--1093},
title = {{Dissecting Characteristics Nonparametrically}},
url = {http://www.nber.org/papers/w23227.pdf},
volume = {10},
year = {2017}
}
@article{Shmueli2010,
annote = {euclid.ss.1294167961.pdf
Contents
* Introduction
Explanatory Modeling
Predictive Modeling
Descriptive Modeling
The Scientific Value of Predictive Modeling
Explaining and Predicting Are Different
A Void in the Statistics Literature
Two Modeling Paths
Study Design and Data Collection
Data Preparation
Handling missing values
Data partitioning
Exploratory Data Analysis
Choice of Variables
Choice of Methods
Validation, Model Evaluation and Model Selection
Validation
Model evaluation
Model selection
Model Use and Reporting
Two Examples
Netflix Prize
Online Auction Research
Implications, Conclusions and Suggestions
The Cost of Indiscrimination to Scientific Research
Explanatory and Predictive Power: Two Dimensions
The Cost of Indiscrimination to the Field of Statistics
Closing Remarks and Suggestions
Appendix: Is the "True" Model the Best Predictive Model? A Linear Regression Example
Acknowledgments
References},
author = {Shmueli, Galit},
doi = {10.1214/10-STS330},
file = {:C$\backslash$:/Users/Administrator/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shmueli - 2010 - To Explain or to Predict.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
language = {en},
month = {aug},
number = {3},
pages = {289--310},
title = {{To Explain or to Predict?}},
url = {http://projecteuclid.org/euclid.ss/1294167961},
volume = {25},
year = {2010}
}
@article{Kozak2017,
abstract = {We construct a robust stochastic discount factor (SDF) that summarizes the joint explanatory power of a large number of cross-sectional stock return predictors. Our method achieves robust out-of-sample performance in this high-dimensional setting by imposing an economically motivated prior on SDF coefficients that shrinks the contributions of low-variance principal components of the candidate factors.  While empirical asset pricing research has focused on SDFs with a small number of characteristics-based factors --- e.g., the four- or five-factor models discussed in the recent literature --- we find that such a characteristics-sparse SDF cannot adequately summarize the cross-section of expected stock returns. However, a relatively small number of principal components of the universe of potential characteristics-based factors can approximate the SDF quite well.},
author = {Kozak, Serhiy and Nagel, Stefan and Santosh, Shrihari},
doi = {10.2139/ssrn.2945663},
file = {:C$\backslash$:/Users/Administrator/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kozak, Nagel, Santosh - 2017 - Shrinking the Cross Section.pdf:pdf},
issn = {1556-5068},
journal = {Ssrn},
keywords = {Cross Section,Factor Models,G11,G12,Machine Learning,SDF,Shrinkage},
pages = {1--69},
title = {{Shrinking the Cross Section}},
year = {2017}
}
@article{Brogaard,
abstract = {Recent advances in machine learning methodologies have improved the usefulness of the technology. This paper examines whether machine learning using only past prices as the input can detect mispricings. Generally searching for mispricings is a slow process and can easily suffer from data-snooping. This paper provides a machine learning algorithm to search for mispricings while controlling for data-snooping. The process generates significant out-of-sample alpha. Overall, the results show that mispricings still exist, but have decreased over time, implying that markets have recently become more efficient.},
author = {Brogaard, Jonathan and Zareei, Abalfazl},
file = {:C$\backslash$:/Users/Administrator/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brogaard, Zareei - Unknown - Machine Learning and the Stock Market.pdf:pdf},
keywords = {Anomalies,Big data analysis,C58,G12,G14,JEL classification: B26,Machine learning,Mispricings,N20 Keywords: Asset pricing},
pages = {1--57},
title = {{Machine Learning and the Stock Market}},
url = {https://ssrn.com/abstract=3233119}
}
@article{Welch2008a,
abstract = {Our article comprehensively reexamines the performance of variables that have been suggested by the academic literature to be good predictors of the equity premium. We find that by and large, these models have predicted poorly both in-sample (IS) and out-of-sample (OOS) for 30 years now; these models seem unstable, as diagnosed by their out-of-sample predictions and other statistics; and these models would not have helped an investor with access only to available information to profitably time the market.},
author = {Welch, Ivo and Goyal, Amit},
doi = {10.1093/rfs/hhm014},
file = {:C$\backslash$:/Users/Administrator/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Welch, Goyal - 2008 - A Comprehensive Look at The Empirical Performance of Equity Premium Prediction.pdf:pdf;:C$\backslash$:/Users/Administrator/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Welch, Goyal - 2008 - A Comprehensive Look at The Empirical Performance of Equity Premium Prediction.html:html},
issn = {0893-9454, 1465-7368},
journal = {Review of Financial Studies},
keywords = {Folder - For Bibfile,Folder - Mutual Funds and tail risk - Bibfile tail,Folder - Tail Risk and Asset Prices - Return Predi,G12,G14},
language = {en},
mendeley-tags = {Folder - For Bibfile,Folder - Mutual Funds and tail risk - Bibfile tail,Folder - Tail Risk and Asset Prices - Return Predi,G12,G14},
month = {jul},
number = {4},
pages = {1455--1508},
title = {{A Comprehensive Look at The Empirical Performance of Equity Premium Prediction}},
url = {http://rfs.oxfordjournals.org/content/21/4/1455 http://rfs.oxfordjournals.org/content/21/4/1455.full.pdf http://rfs.oxfordjournals.org/content/21/4/1455.short},
volume = {21},
year = {2008}
}
@article{Bailey2014,
abstract = {We prove that high simulated performance is easily achievable after backtesting a relatively small number of alternative strategy configurations, a practice we denote “backtest overfitting”. The higher the number of configurations tried, the greater is the probability that the backtest is overfit. Because most financial analysts and academics rarely report the number of configurations tried for a given backtest, investors cannot evaluate the degree of overfitting in most investment proposals. The implication is that investors can be easily misled into allocating capital to strategies that appear to be mathematically sound and empirically supported by an outstanding backtest. Under memory effects, backtest overfitting leads to negative expected returns out-of-sample, rather than zero performance. This may be one of several reasons why so many quantitative funds appear to fail.},
author = {Bailey, David H. and Borwein, Jonathan M. and {L{\'{o}}pez de Prado}, Marcos and Zhu, Qiji Jim},
doi = {10.2139/ssrn.2308659},
file = {:C$\backslash$:/Users/Administrator/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bailey et al. - 2014 - Pseudo-Mathematics and Financial Charlatanism The Effects of Backtest Overfitting on Out-of-Sample Performance.pdf:pdf},
issn = {1556-5068},
journal = {Notices of the AMS},
keywords = {Sharpe ratio,backtest,historical simulation,investment strategy,minimum backtest length,optimization,performance degradation,probability of backtest over-fitting},
number = {5},
pages = {458--471},
title = {{Pseudo-Mathematics and Financial Charlatanism: The Effects of Backtest Overfitting on Out-of-Sample Performance}},
url = {http://papers.ssrn.com/abstract=2308659},
volume = {61},
year = {2014}
}
@article{Pettenuzzo2014,
abstract = {We propose a new approach to imposing economic constraints on time series forecasts of the equity premium. Economic constraints are used to modify the posterior distribution of the parameters of the predictive return regression in a way that better allows the model to learn from the data. We consider two types of constraints: non-negative equity premia and bounds on the conditional Sharpe ratio, the latter of which incorporates time-varying volatility in the predictive regression framework. Empirically, we find that economic constraints systematically reduce uncertainty about model parameters, reduce the risk of selecting a poor forecasting model, and improve both statistical and economic measures of out-of-sample forecast performance.},
author = {Pettenuzzo, Davide and Timmermann, Allan and Valkanov, Rossen},
doi = {10.1016/j.jfineco.2014.07.015},
file = {:C$\backslash$:/Users/Administrator/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pettenuzzo, Timmermann, Valkanov - 2014 - Forecasting stock returns under economic constraints.pdf:pdf},
isbn = {0304-405X},
issn = {0304405X},
journal = {Journal of Financial Economics},
keywords = {Economic constraints,Equity premium predictions,Sharpe ratio},
number = {3},
pages = {517--533},
publisher = {Elsevier},
title = {{Forecasting stock returns under economic constraints}},
url = {http://dx.doi.org/10.1016/j.jfineco.2014.07.015},
volume = {114},
year = {2014}
}
@article{Chinco2018,
author = {Chinco, Alex and Neuhierl, Andreas and Weber, Michael},
file = {:C$\backslash$:/Users/Administrator/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chinco, Neuhierl, Weber - 2018 - Estimating the Anomaly Baserate.pdf:pdf},
keywords = {data mining,penalized regression,return predictability},
pages = {1--30},
title = {{Estimating the Anomaly Baserate}},
year = {2018}
}
@article{Neely2014,
abstract = {Academic research has extensively used macroeconomic variables to forecast the U.S. equity risk premium, with little attention paid to the technical indicators widely employed by practitioners. Our paper fills this gap by comparing the forecasting ability of technical indicators with that of macroeconomic variables. Technical indicators display statistically and economically significant in-sample and out-of-sample forecasting power, matching or exceeding that of macroeconomic variables. Furthermore, technical indicators and macroeconomic variables provide complementary information over the business cycle: technical indicators better detect the typical decline in the equity risk premium near business-cycle peaks, while macroeconomic variables more readily pick up the typical rise in the equity risk premium near cyclical troughs. In line with this behavior, we show that combining information from both technical indicators and macroeconomic variables significantly improves equity risk premium forecasts versus using either type of information alone. Overall, the substantial countercyclical fluctuations in the equity risk premium appear well captured by the combined information in macroeconomic variables and technical indicators.},
author = {Neely, Christopher J. and Rapach, David E. and Tu, Jun and Zhou, Guofu},
doi = {10.1287/mnsc.2013.1838},
file = {:E$\backslash$:/Downloads/mnsc.2013.1838.pdf:pdf},
isbn = {0025-1909},
issn = {0025-1909},
journal = {Management Science},
keywords = {Asset Allocation,C22,C53,E32,Equity Risk Premium Predictability,G11,G12,G17,Macroeconomic Variables,Momentum,Moving-Average Rules,Out-of-Sample Forecasts,Volume},
month = {jul},
number = {7},
pages = {1772--1791},
pmid = {10492251},
title = {{Forecasting the Equity Risk Premium: The Role of Technical Indicators}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/mnsc.2013.1838},
volume = {60},
year = {2014}
}
@article{Rapach2010,
abstract = {Welch and Goyal (2008) find that numerous economic variables with in-sample predictive ability for the equity premium fail to deliver consistent out-of-sample forecasting gains relative to the historical average. Arguing that model uncertainty and instability seriously impair the forecasting ability of individual predictive regression models, we recommend combining individual forecasts. Combining delivers statistically and economically significant out-of-sample gains relative to the historical average consistently over time. We provide two empirical explanations for the benefits of forecast combination: (i) combining forecasts incorporates information from numerous economic variables while substantially reducing forecast volatility; (ii) combination forecasts are linked to the real economy.},
author = {Rapach, David E. and Strauss, Jack K. and Zhou, Guofu},
doi = {10.1093/rfs/hhp063},
file = {:C$\backslash$:/Users/Administrator/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rapach, Strauss, Zhou - 2010 - Out-of-sample equity premium prediction Combination forecasts and links to the real economy.pdf:pdf},
isbn = {08939454},
issn = {08939454},
journal = {Review of Financial Studies},
number = {2},
pages = {821--862},
title = {{Out-of-sample equity premium prediction: Combination forecasts and links to the real economy}},
volume = {23},
year = {2010}
}
@article{Heaton2017,
abstract = {We explore the use of deep learning hierarchical models for problems in financial prediction and classification. Financial prediction problems – such as those presented in designing and pricing securities, constructing portfolios, and risk management – often involve large data sets with complex data interactions that currently are difficult or impossible to specify in a full economic model. Applying deep learning methods to these problems can produce more useful results than standard methods in finance. In particular, deep learning can detect and exploit interactions in the data that are, at least currently, invisible to any existing financial economic theory.},
author = {Heaton, J. B. and Polson, N. G. and Witte, J. H.},
doi = {10.1002/asmb.2209},
file = {:C$\backslash$:/Users/Administrator/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Heaton, Polson, Witte - 2017 - Deep learning for finance deep portfolios.pdf:pdf},
isbn = {15241904},
issn = {15264025},
journal = {Applied Stochastic Models in Business and Industry},
keywords = {artificial intelligence,asset pricing,big data,deep frontier,deep learning,finance,machine learning,volatility},
number = {1},
pages = {3--12},
title = {{Deep learning for finance: deep portfolios}},
volume = {33},
year = {2017}
}
@article{rapach_short_2016,
abstract = {We show that short interest is arguably the strongest known predictor of aggregate stock returns. It outperforms a host of popular return predictors both in and out of sample, with annual R2 statistics of 12.89{\%} and 13.24{\%}, respectively. In addition, short interest can generate utility gains of over 300 basis points per annum for a mean-variance investor. A vector autoregression decomposition shows that the economic source of short interest's predictive power stems predominantly from a cash flow channel. Overall, our evidence indicates that short sellers are informed traders who are able to anticipate future aggregate cash flows and associated market returns.},
author = {Rapach, David E and Ringgenberg, Matthew C and Zhou, Guofu},
doi = {10.1016/j.jfineco.2016.03.004},
issn = {0304-405X},
journal = {Journal of Financial Economics},
keywords = {Cash flow channel,Equity risk premium,Informed traders,Predictive regression,Short interest},
month = {jul},
number = {1},
pages = {46--65},
title = {{Short interest and aggregate stock returns}},
url = {http://www.sciencedirect.com/science/article/pii/S0304405X16300320},
volume = {121},
year = {2016}
}
@article{Kleinberg2015,
abstract = {Empirical policy research often focuses on causal inference. Since policy choices seem to depend on understanding the counterfactual— what happens with and without a policy—this tight link of causality and policy seems natural. While this link holds in many cases, we argue that there are also many policy applications where causal inference is not central, or even necessary. Consider two toy examples. One policymaker facing a drought must decide whether to invest in a rain dance to increase the chance of rain. Another seeing clouds must decide whether to take an umbrella to work to avoid getting wet on the way home. Both decisions could benefit from an empirical study of rain. But each has differ-ent requirements of the estimator. One requires causality: Do rain dances cause rain? The other does not, needing only prediction: Is the chance of rain high enough to merit an umbrella? We often focus on rain dance–like policy problems. But there are also many umbrella-like policy problems. Not only are these prediction prob-lems neglected, machine learning can help us solve them more effectively. In this paper, we (i) provide a simple frame-work that clarifies the distinction between causation and prediction; (ii) explain how machine learning adds value over traditional regression approaches in solving prediction problems; (iii) provide an empirical example from health policy to illustrate how improved predictions can generate large social impact; (iv) illustrate how " umbrella " problems are common and important in many important pol-icy domains; and (v) argue that solving these problems produces not just policy impact but also theoretical and economic insights. 1 I. Prediction and Causation Let Y be an outcome variable (such as rain) which depends in an unknown way on a set of variables X 0 and X . A policymaker must decide on X 0 (e.g., an umbrella or rain dance) in order to maximize a (known) payoff function $\pi$(X 0 , Y) . Our decision of X 0 depends on the derivative d$\pi$(X 0 , Y)},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Kleinberg, Jon and Ludwig, Jens and Mullainathan, Sendhil and Obermeyer, Ziad},
doi = {10.1257/aer.p20151023},
eprint = {15334406},
file = {:E$\backslash$:/Downloads/aer.p20151023.pdf:pdf},
isbn = {0002-8282},
issn = {00106178},
journal = {American Economic Review},
number = {5},
pages = {491--495},
pmid = {24655651},
title = {{Prediction Policy Problems}},
url = {http://pubs.aeaweb.org/doi/10.1257/aer.p20151023},
volume = {105},
year = {2015}
}
@article{Belloni2010,
abstract = {In this paper we study post-model selection estimators which apply ordinary least squares (ols) to the model selected by first-step penalized estimators, typically lasso. It is well known that lasso can estimate the non-parametric regression function at nearly the oracle rate, and is thus hard to improve upon. We show that ols post lasso estimator performs at least as well as lasso in terms of the rate of convergence, and has the advantage of a smaller bias. Remarkably, this performance occurs even if the lasso-based model selection “fails” in the sense of missing some components of the “true” regression model. By the “true” model we mean here the best s-dimensional approximation to the nonparametric regression function chosen by the oracle. Furthermore, ols post lasso estimator can perform strictly better than lasso, in the sense of a strictly faster rate of convergence, if the lasso-based model selection correctly includes all components of the “true” model as a subset and also achieves sufficient sparsity. In the extreme case, when lasso perfectly selects the “true” model, the ols post lasso estimator becomes the oracle estimator. An important ingredient in our analysis is a new sparsity bound on the dimension of the model selected by lasso which guarantees that this dimension is at most of the same order as the dimension of the “true” model. Our rate results are non-asymptotic and hold in both parametric and nonparametric models. Moreover, our analysis is not limited to the lasso estimator acting as selector in the first step, but also applies to any other estimator, for example various forms of thresholded lasso, with good rates and good sparsity properties. Our analysis covers both traditional thresholding and a new practical, data-driven thresholding scheme that induces maximal sparsity subject to maintaining a certain goodness-of-fit. The latter scheme has theoretical guarantees similar to those of lasso or ols post lasso, but it dominates these procedures as well as traditional thresholding in a wide variety of experiments.    First arXiv version: December 2009.},
archivePrefix = {arXiv},
arxivId = {1001.0188},
author = {Belloni, Alexandre and Chernozhukov, Victor},
doi = {10.2139/ssrn.1582594},
eprint = {1001.0188},
file = {:E$\backslash$:/Downloads/euclid.bj.1363192037.pdf:pdf},
issn = {1350-7265},
journal = {Ssrn},
keywords = {H12,J07,J99,lasso,ols post lasso,post-model-selection estimators},
number = {2},
pages = {521--547},
pmid = {10694730},
title = {{Least Squares After Model Selection in High-Dimensional Sparse Models}},
volume = {19},
year = {2010}
}
@article{Belloni2017,
abstract = {In this paper, we provide efficient estimators and honest confidence bands for a variety of treatment effects including local average (LATE) and local quantile treatment effects (LQTE) in data-rich environments. We can handle very many control variables, endogenous receipt of treatment, heterogeneous treatment effects, and function-valued outcomes. Our framework covers the special case of exogenous receipt of treatment, either conditional on controls or unconditionally as in randomized control trials. In the latter case, our approach produces efficient estimators and honest bands for (functional) average treatment effects (ATE) and quantile treatment effects (QTE). To make informative inference possible, we assume that key reduced form predictive relationships are approximately sparse. This assumption allows the use of regularization and selection methods to estimate those relations, and we provide methods for post-regularization and post-selection inference that are uniformly valid (honest) across a wide-range of models. We show that a key ingredient enabling honest inference is the use of orthogonal or doubly robust moment conditions in estimating certain reduced form functional parameters. We illustrate the use of the proposed methods with an application to estimating the effect of 401(k) eligibility and participation on accumulated assets.},
archivePrefix = {arXiv},
arxivId = {1311.2645},
author = {Belloni, Alexandre and Chernozhukov, Victor and Fernandez-Val, I. and Hansen, Christian},
doi = {10.3982/ECTA12723},
eprint = {1311.2645},
file = {:E$\backslash$:/Downloads/Belloni{\_}et{\_}al-2017-Econometrica.pdf:pdf},
isbn = {1529-2401 (Electronic)$\backslash$r0270-6474 (Linking)},
issn = {0012-9682},
journal = {Econometrica},
number = {1},
pages = {233--298},
pmid = {17360903},
title = {{Program Evaluation and Causal Inference With High-Dimensional Data}},
url = {http://arxiv.org/abs/1311.2645 https://www.econometricsociety.org/doi/10.3982/ECTA12723},
volume = {85},
year = {2017}
}
@article{Chernozhukov2018,
abstract = {We revisit the classic semi{\&}{\#}8208;parametric problem of inference on a low{\&}{\#}8208;dimensional parameter {\&}{\#}952; in the presence of high{\&}{\#}8208;dimensional nuisance parameters {\&}{\#}951;. We depart from the classical setting by allowing for {\&}{\#}951; to be so high{\&}{\#}8208;dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate {\&}{\#}951;, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high{\&}{\#}8208;dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating {\&}{\#}951; cause a heavy bias in estimators of {\&}{\#}952; that are obtained by naively plugging ML estimators of {\&}{\#}951; into estimating equations for {\&}{\#}952;. This bias results in the naive estimator failing to be  consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest {\&}{\#}952; can be removed by using two simple, yet critical, ingredients: (1) using Neyman{\&}{\#}8208;orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate {\&}{\#}952;; (2) making use of cross{\&}{\#}8208;fitting, which provides an efficient form of data{\&}{\#}8208;splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an {\&}{\#}8208;neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.},
archivePrefix = {arXiv},
arxivId = {1608.00060},
author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
doi = {10.1111/ectj.12097},
eprint = {1608.00060},
file = {:E$\backslash$:/Downloads/Chernozhukov{\_}et{\_}al-2018-The{\_}Econometrics{\_}Journal.pdf:pdf},
issn = {1368423X},
journal = {Econometrics Journal},
number = {1},
pages = {C1--C68},
title = {{Double/debiased machine learning for treatment and structural parameters}},
volume = {21},
year = {2018}
}
@article{Breiman2001,
abstract = {There are two cultures in the use of statistical modeling to reach$\backslash$nconclusions from data. One assumes that the data are generated by$\backslash$na given stochastic data model. The other uses algorithmic models$\backslash$nand treats the data mechanism as unknown. The statistical community$\backslash$nhas been committed to the almost exclusive use of data models. This$\backslash$ncommitment has led to irrelevant theory, questionable conclusions,$\backslash$nand has kept statisticians from working on a large range of interesting$\backslash$ncurrent problems. Algorithmic modeling, both in theory and practice,$\backslash$nhas developed rapidly in fields outside statistics. It can be used$\backslash$nboth on large complex data sets and as a more accurate and informative$\backslash$nalternative to data modeling on smaller data sets. If our goal as$\backslash$na field is to use data to solve problems, then we need to move away$\backslash$nfrom exclusive dependence on data models and adopt a more diverse$\backslash$nset of tools.},
author = {Breiman, Leo},
doi = {10.1214/ss/1009213726},
file = {:C$\backslash$:/Users/Administrator/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Breiman - 2001 - {\{}S{\}}tatistical {\{}M{\}}odeling {\{}T{\}}he {\{}T{\}}wo {\{}C{\}}ultures.pdf:pdf},
journal = {Statistical Science},
number = {3},
pages = {199--309},
title = {{{\{}S{\}}tatistical {\{}M{\}}odeling: {\{}T{\}}he {\{}T{\}}wo {\{}C{\}}ultures}},
volume = {16},
year = {2001}
}
